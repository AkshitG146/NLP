{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text ='''Neural Networks are set of algorithms which closely resemble the human brain and are designed to recognize patterns. They interpret sensory data through a machine perception, labelling or clustering raw input. They can recognize numerical patterns, contained in vectors, into which all real-world data ( images, sound, text or time series), must be translated. Artificial neural networks are composed of a large number of highly interconnected processing elements (neuron) working together to solve a problem.\n",
    "\n",
    "An ANN usually involves a large number of processors operating in parallel and arranged in tiers. The first tier receives the raw input information — analogous to optic nerves in human visual processing. Each successive tier receives the output from the tier preceding it, rather than from the raw input — in the same way neurons further from the optic nerve receive signals from those closer to it. The last tier produces the output of the system.. Recurrent Neural Network is a generalization of feedforward neural network that has an internal memory. RNN is recurrent in nature as it performs the same function for every input of data while the output of the current input depends on the past one computation. After producing the output, it is copied and sent back into the recurrent network. For making a decision, it considers the current input and the output that it has learned from the previous input.\n",
    "\n",
    "Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. In other neural networks, all the inputs are independent of each other. But in RNN, all the inputs are related to each other. This helps the RNNs to exhibit temporal dynamic behavior. The memory of the RNN is not pre-determined. It is a variable that evolves over time.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neural Networks are set of algorithms which closely resemble the human brain and are designed to recognize patterns. They interpret sensory data through a machine perception, labelling or clustering raw input. They can recognize numerical patterns, contained in vectors, into which all real-world data ( images, sound, text or time series), must be translated. Artificial neural networks are composed of a large number of highly interconnected processing elements (neuron) working together to solve a problem.\\n\\nAn ANN usually involves a large number of processors operating in parallel and arranged in tiers. The first tier receives the raw input information — analogous to optic nerves in human visual processing. Each successive tier receives the output from the tier preceding it, rather than from the raw input — in the same way neurons further from the optic nerve receive signals from those closer to it. The last tier produces the output of the system.. Recurrent Neural Network is a generalization of feedforward neural network that has an internal memory. RNN is recurrent in nature as it performs the same function for every input of data while the output of the current input depends on the past one computation. After producing the output, it is copied and sent back into the recurrent network. For making a decision, it considers the current input and the output that it has learned from the previous input.\\n\\nUnlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. In other neural networks, all the inputs are independent of each other. But in RNN, all the inputs are related to each other. This helps the RNNs to exhibit temporal dynamic behavior. The memory of the RNN is not pre-determined. It is a variable that evolves over time.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "sentences= nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Neural Networks are set of algorithms which closely resemble the human brain and are designed to recognize patterns.',\n",
       " 'They interpret sensory data through a machine perception, labelling or clustering raw input.',\n",
       " 'They can recognize numerical patterns, contained in vectors, into which all real-world data ( images, sound, text or time series), must be translated.',\n",
       " 'Artificial neural networks are composed of a large number of highly interconnected processing elements (neuron) working together to solve a problem.',\n",
       " 'An ANN usually involves a large number of processors operating in parallel and arranged in tiers.',\n",
       " 'The first tier receives the raw input information — analogous to optic nerves in human visual processing.',\n",
       " 'Each successive tier receives the output from the tier preceding it, rather than from the raw input — in the same way neurons further from the optic nerve receive signals from those closer to it.',\n",
       " 'The last tier produces the output of the system.. Recurrent Neural Network is a generalization of feedforward neural network that has an internal memory.',\n",
       " 'RNN is recurrent in nature as it performs the same function for every input of data while the output of the current input depends on the past one computation.',\n",
       " 'After producing the output, it is copied and sent back into the recurrent network.',\n",
       " 'For making a decision, it considers the current input and the output that it has learned from the previous input.',\n",
       " 'Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs.',\n",
       " 'This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition.',\n",
       " 'In other neural networks, all the inputs are independent of each other.',\n",
       " 'But in RNN, all the inputs are related to each other.',\n",
       " 'This helps the RNNs to exhibit temporal dynamic behavior.',\n",
       " 'The memory of the RNN is not pre-determined.',\n",
       " 'It is a variable that evolves over time.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA CLEANING\n",
    "\n",
    "import re\n",
    "corpus=[]\n",
    "for i in range(len(sentences)):\n",
    "    review= re.sub('[^a-zA-Z]',' ',sentences[i])\n",
    "    review= review.lower()\n",
    "    review= review.split()\n",
    "    review= ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neural networks are set of algorithms which closely resemble the human brain and are designed to recognize patterns',\n",
       " 'they interpret sensory data through a machine perception labelling or clustering raw input',\n",
       " 'they can recognize numerical patterns contained in vectors into which all real world data images sound text or time series must be translated',\n",
       " 'artificial neural networks are composed of a large number of highly interconnected processing elements neuron working together to solve a problem',\n",
       " 'an ann usually involves a large number of processors operating in parallel and arranged in tiers',\n",
       " 'the first tier receives the raw input information analogous to optic nerves in human visual processing',\n",
       " 'each successive tier receives the output from the tier preceding it rather than from the raw input in the same way neurons further from the optic nerve receive signals from those closer to it',\n",
       " 'the last tier produces the output of the system recurrent neural network is a generalization of feedforward neural network that has an internal memory',\n",
       " 'rnn is recurrent in nature as it performs the same function for every input of data while the output of the current input depends on the past one computation',\n",
       " 'after producing the output it is copied and sent back into the recurrent network',\n",
       " 'for making a decision it considers the current input and the output that it has learned from the previous input',\n",
       " 'unlike feedforward neural networks rnns can use their internal state memory to process sequences of inputs',\n",
       " 'this makes them applicable to tasks such as unsegmented connected handwriting recognition or speech recognition',\n",
       " 'in other neural networks all the inputs are independent of each other',\n",
       " 'but in rnn all the inputs are related to each other',\n",
       " 'this helps the rnns to exhibit temporal dynamic behavior',\n",
       " 'the memory of the rnn is not pre determined',\n",
       " 'it is a variable that evolves over time']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural\n",
      "network\n",
      "set\n",
      "algorithm\n",
      "close\n",
      "resembl\n",
      "human\n",
      "brain\n",
      "design\n",
      "recogn\n",
      "pattern\n",
      "interpret\n",
      "sensori\n",
      "data\n",
      "machin\n",
      "percept\n",
      "label\n",
      "cluster\n",
      "raw\n",
      "input\n",
      "recogn\n",
      "numer\n",
      "pattern\n",
      "contain\n",
      "vector\n",
      "real\n",
      "world\n",
      "data\n",
      "imag\n",
      "sound\n",
      "text\n",
      "time\n",
      "seri\n",
      "must\n",
      "translat\n",
      "artifici\n",
      "neural\n",
      "network\n",
      "compos\n",
      "larg\n",
      "number\n",
      "highli\n",
      "interconnect\n",
      "process\n",
      "element\n",
      "neuron\n",
      "work\n",
      "togeth\n",
      "solv\n",
      "problem\n",
      "ann\n",
      "usual\n",
      "involv\n",
      "larg\n",
      "number\n",
      "processor\n",
      "oper\n",
      "parallel\n",
      "arrang\n",
      "tier\n",
      "first\n",
      "tier\n",
      "receiv\n",
      "raw\n",
      "input\n",
      "inform\n",
      "analog\n",
      "optic\n",
      "nerv\n",
      "human\n",
      "visual\n",
      "process\n",
      "success\n",
      "tier\n",
      "receiv\n",
      "output\n",
      "tier\n",
      "preced\n",
      "rather\n",
      "raw\n",
      "input\n",
      "way\n",
      "neuron\n",
      "optic\n",
      "nerv\n",
      "receiv\n",
      "signal\n",
      "closer\n",
      "last\n",
      "tier\n",
      "produc\n",
      "output\n",
      "system\n",
      "recurr\n",
      "neural\n",
      "network\n",
      "gener\n",
      "feedforward\n",
      "neural\n",
      "network\n",
      "intern\n",
      "memori\n",
      "rnn\n",
      "recurr\n",
      "natur\n",
      "perform\n",
      "function\n",
      "everi\n",
      "input\n",
      "data\n",
      "output\n",
      "current\n",
      "input\n",
      "depend\n",
      "past\n",
      "one\n",
      "comput\n",
      "produc\n",
      "output\n",
      "copi\n",
      "sent\n",
      "back\n",
      "recurr\n",
      "network\n",
      "make\n",
      "decis\n",
      "consid\n",
      "current\n",
      "input\n",
      "output\n",
      "learn\n",
      "previou\n",
      "input\n",
      "unlik\n",
      "feedforward\n",
      "neural\n",
      "network\n",
      "rnn\n",
      "use\n",
      "intern\n",
      "state\n",
      "memori\n",
      "process\n",
      "sequenc\n",
      "input\n",
      "make\n",
      "applic\n",
      "task\n",
      "unseg\n",
      "connect\n",
      "handwrit\n",
      "recognit\n",
      "speech\n",
      "recognit\n",
      "neural\n",
      "network\n",
      "input\n",
      "independ\n",
      "rnn\n",
      "input\n",
      "relat\n",
      "help\n",
      "rnn\n",
      "exhibit\n",
      "tempor\n",
      "dynam\n",
      "behavior\n",
      "memori\n",
      "rnn\n",
      "pre\n",
      "determin\n",
      "variabl\n",
      "evolv\n",
      "time\n"
     ]
    }
   ],
   "source": [
    "# STEMMING\n",
    "\n",
    "\n",
    "ps= PorterStemmer()\n",
    "for i in corpus:\n",
    "    words= nltk.word_tokenize(i)\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\akshi\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural\n",
      "network\n",
      "set\n",
      "algorithm\n",
      "closely\n",
      "resemble\n",
      "human\n",
      "brain\n",
      "designed\n",
      "recognize\n",
      "pattern\n",
      "interpret\n",
      "sensory\n",
      "data\n",
      "machine\n",
      "perception\n",
      "labelling\n",
      "clustering\n",
      "raw\n",
      "input\n",
      "recognize\n",
      "numerical\n",
      "pattern\n",
      "contained\n",
      "vector\n",
      "real\n",
      "world\n",
      "data\n",
      "image\n",
      "sound\n",
      "text\n",
      "time\n",
      "series\n",
      "must\n",
      "translated\n",
      "artificial\n",
      "neural\n",
      "network\n",
      "composed\n",
      "large\n",
      "number\n",
      "highly\n",
      "interconnected\n",
      "processing\n",
      "element\n",
      "neuron\n",
      "working\n",
      "together\n",
      "solve\n",
      "problem\n",
      "ann\n",
      "usually\n",
      "involves\n",
      "large\n",
      "number\n",
      "processor\n",
      "operating\n",
      "parallel\n",
      "arranged\n",
      "tier\n",
      "first\n",
      "tier\n",
      "receives\n",
      "raw\n",
      "input\n",
      "information\n",
      "analogous\n",
      "optic\n",
      "nerve\n",
      "human\n",
      "visual\n",
      "processing\n",
      "successive\n",
      "tier\n",
      "receives\n",
      "output\n",
      "tier\n",
      "preceding\n",
      "rather\n",
      "raw\n",
      "input\n",
      "way\n",
      "neuron\n",
      "optic\n",
      "nerve\n",
      "receive\n",
      "signal\n",
      "closer\n",
      "last\n",
      "tier\n",
      "produce\n",
      "output\n",
      "system\n",
      "recurrent\n",
      "neural\n",
      "network\n",
      "generalization\n",
      "feedforward\n",
      "neural\n",
      "network\n",
      "internal\n",
      "memory\n",
      "rnn\n",
      "recurrent\n",
      "nature\n",
      "performs\n",
      "function\n",
      "every\n",
      "input\n",
      "data\n",
      "output\n",
      "current\n",
      "input\n",
      "depends\n",
      "past\n",
      "one\n",
      "computation\n",
      "producing\n",
      "output\n",
      "copied\n",
      "sent\n",
      "back\n",
      "recurrent\n",
      "network\n",
      "making\n",
      "decision\n",
      "considers\n",
      "current\n",
      "input\n",
      "output\n",
      "learned\n",
      "previous\n",
      "input\n",
      "unlike\n",
      "feedforward\n",
      "neural\n",
      "network\n",
      "rnns\n",
      "use\n",
      "internal\n",
      "state\n",
      "memory\n",
      "process\n",
      "sequence\n",
      "input\n",
      "make\n",
      "applicable\n",
      "task\n",
      "unsegmented\n",
      "connected\n",
      "handwriting\n",
      "recognition\n",
      "speech\n",
      "recognition\n",
      "neural\n",
      "network\n",
      "input\n",
      "independent\n",
      "rnn\n",
      "input\n",
      "related\n",
      "help\n",
      "rnns\n",
      "exhibit\n",
      "temporal\n",
      "dynamic\n",
      "behavior\n",
      "memory\n",
      "rnn\n",
      "pre\n",
      "determined\n",
      "variable\n",
      "evolves\n",
      "time\n"
     ]
    }
   ],
   "source": [
    "# LEMMATIZATION\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "npl= WordNetLemmatizer()\n",
    "for i in corpus:\n",
    "    words= nltk.word_tokenize(i)\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            print(npl.lemmatize(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEMMATIZATION USING TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"da_dacy_large_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Neural, Lemma: Neural\n",
      "Token: Networks, Lemma: Networks\n",
      "Token: are, Lemma: are\n",
      "Token: set, Lemma: set\n",
      "Token: of, Lemma: of\n",
      "Token: algorithms, Lemma: algorithm\n",
      "Token: which, Lemma: which\n",
      "Token: closely, Lemma: closely\n",
      "Token: resemble, Lemma: resemble\n",
      "Token: the, Lemma: the\n",
      "Token: human, Lemma: human\n",
      "Token: brain, Lemma: brain\n",
      "Token: and, Lemma: and\n",
      "Token: are, Lemma: are\n",
      "Token: designed, Lemma: designed\n",
      "Token: to, Lemma: to\n",
      "Token: recognize, Lemma: recognize\n",
      "Token: patterns, Lemma: patterns\n",
      "Token: ., Lemma: .\n",
      "Token: They, Lemma: They\n",
      "Token: interpret, Lemma: interpret\n",
      "Token: sensory, Lemma: sensory\n",
      "Token: data, Lemma: data\n",
      "Token: through, Lemma: through\n",
      "Token: a, Lemma: a\n",
      "Token: machine, Lemma: machine\n",
      "Token: perception, Lemma: perception\n",
      "Token: ,, Lemma: ,\n",
      "Token: labelling, Lemma: labelling\n",
      "Token: or, Lemma: or\n",
      "Token: clustering, Lemma: clustering\n",
      "Token: raw, Lemma: raw\n",
      "Token: input, Lemma: input\n",
      "Token: ., Lemma: .\n",
      "Token: They, Lemma: They\n",
      "Token: can, Lemma: cunne\n",
      "Token: recognize, Lemma: recognize\n",
      "Token: numerical, Lemma: numerical\n",
      "Token: patterns, Lemma: patterns\n",
      "Token: ,, Lemma: ,\n",
      "Token: contained, Lemma: contained\n",
      "Token: in, Lemma: in\n",
      "Token: vectors, Lemma: vectors\n",
      "Token: ,, Lemma: ,\n",
      "Token: into, Lemma: into\n",
      "Token: which, Lemma: which\n",
      "Token: all, Lemma: all\n",
      "Token: real-world, Lemma: real-world\n",
      "Token: data, Lemma: data\n",
      "Token: (, Lemma: (\n",
      "Token: images, Lemma: image\n",
      "Token: ,, Lemma: ,\n",
      "Token: sound, Lemma: sound\n",
      "Token: ,, Lemma: ,\n",
      "Token: text, Lemma: text\n",
      "Token: or, Lemma: or\n",
      "Token: time, Lemma: time\n",
      "Token: series, Lemma: series\n",
      "Token: ), Lemma: )\n",
      "Token: ,, Lemma: ,\n",
      "Token: must, Lemma: musttte\n",
      "Token: be, Lemma: be\n",
      "Token: translated, Lemma: translated\n",
      "Token: ., Lemma: .\n",
      "Token: Artificial, Lemma: Artificial\n",
      "Token: neural, Lemma: neural\n",
      "Token: networks, Lemma: networks\n",
      "Token: are, Lemma: are\n",
      "Token: composed, Lemma: composed\n",
      "Token: of, Lemma: of\n",
      "Token: a, Lemma: a\n",
      "Token: large, Lemma: large\n",
      "Token: number, Lemma: number\n",
      "Token: of, Lemma: of\n",
      "Token: highly, Lemma: highly\n",
      "Token: interconnected, Lemma: interconnected\n",
      "Token: processing, Lemma: processing\n",
      "Token: elements, Lemma: element\n",
      "Token: (, Lemma: (\n",
      "Token: neuron, Lemma: neuron\n",
      "Token: ), Lemma: )\n",
      "Token: working, Lemma: working\n",
      "Token: together, Lemma: together\n",
      "Token: to, Lemma: to\n",
      "Token: solve, Lemma: solve\n",
      "Token: a, Lemma: a\n",
      "Token: problem, Lemma: problem\n",
      "Token: ., Lemma: .\n",
      "Token: \n",
      "\n",
      ", Lemma: \n",
      "\n",
      "\n",
      "Token: An, Lemma: An\n",
      "Token: ANN, Lemma: ANN\n",
      "Token: usually, Lemma: usually\n",
      "Token: involves, Lemma: involves\n",
      "Token: a, Lemma: a\n",
      "Token: large, Lemma: large\n",
      "Token: number, Lemma: number\n",
      "Token: of, Lemma: of\n",
      "Token: processors, Lemma: processors\n",
      "Token: operating, Lemma: operating\n",
      "Token: in, Lemma: in\n",
      "Token: parallel, Lemma: parallel\n",
      "Token: and, Lemma: and\n",
      "Token: arranged, Lemma: arranged\n",
      "Token: in, Lemma: in\n",
      "Token: tiers, Lemma: tiers\n",
      "Token: ., Lemma: .\n",
      "Token: The, Lemma: The\n",
      "Token: first, Lemma: first\n",
      "Token: tier, Lemma: tier\n",
      "Token: receives, Lemma: receives\n",
      "Token: the, Lemma: the\n",
      "Token: raw, Lemma: raw\n",
      "Token: input, Lemma: input\n",
      "Token: information, Lemma: information\n",
      "Token: —, Lemma: —\n",
      "Token: analogous, Lemma: analogous\n",
      "Token: to, Lemma: to\n",
      "Token: optic, Lemma: optic\n",
      "Token: nerves, Lemma: nerves\n",
      "Token: in, Lemma: in\n",
      "Token: human, Lemma: human\n",
      "Token: visual, Lemma: visual\n",
      "Token: processing, Lemma: processing\n",
      "Token: ., Lemma: .\n",
      "Token: Each, Lemma: Each\n",
      "Token: successive, Lemma: successive\n",
      "Token: tier, Lemma: tier\n",
      "Token: receives, Lemma: receives\n",
      "Token: the, Lemma: the\n",
      "Token: output, Lemma: output\n",
      "Token: from, Lemma: from\n",
      "Token: the, Lemma: the\n",
      "Token: tier, Lemma: tier\n",
      "Token: preceding, Lemma: preceding\n",
      "Token: it, Lemma: it\n",
      "Token: ,, Lemma: ,\n",
      "Token: rather, Lemma: rather\n",
      "Token: than, Lemma: than\n",
      "Token: from, Lemma: from\n",
      "Token: the, Lemma: the\n",
      "Token: raw, Lemma: raw\n",
      "Token: input, Lemma: input\n",
      "Token: —, Lemma: —\n",
      "Token: in, Lemma: in\n",
      "Token: the, Lemma: the\n",
      "Token: same, Lemma: same\n",
      "Token: way, Lemma: way\n",
      "Token: neurons, Lemma: neurons\n",
      "Token: further, Lemma: further\n",
      "Token: from, Lemma: from\n",
      "Token: the, Lemma: the\n",
      "Token: optic, Lemma: optic\n",
      "Token: nerve, Lemma: nerve\n",
      "Token: receive, Lemma: receive\n",
      "Token: signals, Lemma: signals\n",
      "Token: from, Lemma: from\n",
      "Token: those, Lemma: thosen\n",
      "Token: closer, Lemma: closer\n",
      "Token: to, Lemma: to\n",
      "Token: it., Lemma: it.\n",
      "Token: The, Lemma: The\n",
      "Token: last, Lemma: last\n",
      "Token: tier, Lemma: tier\n",
      "Token: produces, Lemma: produces\n",
      "Token: the, Lemma: the\n",
      "Token: output, Lemma: output\n",
      "Token: of, Lemma: of\n",
      "Token: the, Lemma: the\n",
      "Token: system, Lemma: system\n",
      "Token: .., Lemma: ..\n",
      "Token: Recurrent, Lemma: Recurrent\n",
      "Token: Neural, Lemma: Neural\n",
      "Token: Network, Lemma: Network\n",
      "Token: is, Lemma: is\n",
      "Token: a, Lemma: a\n",
      "Token: generalization, Lemma: generalization\n",
      "Token: of, Lemma: of\n",
      "Token: feedforward, Lemma: feedforward\n",
      "Token: neural, Lemma: neural\n",
      "Token: network, Lemma: network\n",
      "Token: that, Lemma: that\n",
      "Token: has, Lemma: has\n",
      "Token: an, Lemma: an\n",
      "Token: internal, Lemma: internal\n",
      "Token: memory, Lemma: memory\n",
      "Token: ., Lemma: .\n",
      "Token: RNN, Lemma: RNN\n",
      "Token: is, Lemma: is\n",
      "Token: recurrent, Lemma: recurrent\n",
      "Token: in, Lemma: in\n",
      "Token: nature, Lemma: nature\n",
      "Token: as, Lemma: as\n",
      "Token: it, Lemma: it\n",
      "Token: performs, Lemma: performs\n",
      "Token: the, Lemma: the\n",
      "Token: same, Lemma: same\n",
      "Token: function, Lemma: function\n",
      "Token: for, Lemma: for\n",
      "Token: every, Lemma: every\n",
      "Token: input, Lemma: input\n",
      "Token: of, Lemma: of\n",
      "Token: data, Lemma: data\n",
      "Token: while, Lemma: while\n",
      "Token: the, Lemma: the\n",
      "Token: output, Lemma: output\n",
      "Token: of, Lemma: of\n",
      "Token: the, Lemma: the\n",
      "Token: current, Lemma: current\n",
      "Token: input, Lemma: input\n",
      "Token: depends, Lemma: depends\n",
      "Token: on, Lemma: on\n",
      "Token: the, Lemma: the\n",
      "Token: past, Lemma: past\n",
      "Token: one, Lemma: one\n",
      "Token: computation, Lemma: computation\n",
      "Token: ., Lemma: .\n",
      "Token: After, Lemma: After\n",
      "Token: producing, Lemma: producing\n",
      "Token: the, Lemma: the\n",
      "Token: output, Lemma: output\n",
      "Token: ,, Lemma: ,\n",
      "Token: it, Lemma: it\n",
      "Token: is, Lemma: is\n",
      "Token: copied, Lemma: copied\n",
      "Token: and, Lemma: and\n",
      "Token: sent, Lemma: sene\n",
      "Token: back, Lemma: back\n",
      "Token: into, Lemma: into\n",
      "Token: the, Lemma: the\n",
      "Token: recurrent, Lemma: recurrent\n",
      "Token: network, Lemma: network\n",
      "Token: ., Lemma: .\n",
      "Token: For, Lemma: for\n",
      "Token: making, Lemma: making\n",
      "Token: a, Lemma: a\n",
      "Token: decision, Lemma: decision\n",
      "Token: ,, Lemma: ,\n",
      "Token: it, Lemma: it\n",
      "Token: considers, Lemma: considers\n",
      "Token: the, Lemma: the\n",
      "Token: current, Lemma: current\n",
      "Token: input, Lemma: input\n",
      "Token: and, Lemma: and\n",
      "Token: the, Lemma: the\n",
      "Token: output, Lemma: output\n",
      "Token: that, Lemma: that\n",
      "Token: it, Lemma: it\n",
      "Token: has, Lemma: has\n",
      "Token: learned, Lemma: learned\n",
      "Token: from, Lemma: from\n",
      "Token: the, Lemma: the\n",
      "Token: previous, Lemma: previous\n",
      "Token: input, Lemma: input\n",
      "Token: ., Lemma: .\n",
      "Token: \n",
      "\n",
      ", Lemma: \n",
      "\n",
      "\n",
      "Token: Unlike, Lemma: Unlike\n",
      "Token: feedforward, Lemma: feedforward\n",
      "Token: neural, Lemma: neural\n",
      "Token: networks, Lemma: networks\n",
      "Token: ,, Lemma: ,\n",
      "Token: RNNs, Lemma: RNNs\n",
      "Token: can, Lemma: cunne\n",
      "Token: use, Lemma: use\n",
      "Token: their, Lemma: their\n",
      "Token: internal, Lemma: internal\n",
      "Token: state, Lemma: state\n",
      "Token: (, Lemma: (\n",
      "Token: memory, Lemma: memory\n",
      "Token: ), Lemma: )\n",
      "Token: to, Lemma: to\n",
      "Token: process, Lemma: process\n",
      "Token: sequences, Lemma: sequences\n",
      "Token: of, Lemma: of\n",
      "Token: inputs, Lemma: inputs\n",
      "Token: ., Lemma: .\n",
      "Token: This, Lemma: This\n",
      "Token: makes, Lemma: makes\n",
      "Token: them, Lemma: the\n",
      "Token: applicable, Lemma: applicable\n",
      "Token: to, Lemma: to\n",
      "Token: tasks, Lemma: tasks\n",
      "Token: such, Lemma: such\n",
      "Token: as, Lemma: as\n",
      "Token: unsegmented, Lemma: unsegmented\n",
      "Token: ,, Lemma: ,\n",
      "Token: connected, Lemma: connected\n",
      "Token: handwriting, Lemma: handwriting\n",
      "Token: recognition, Lemma: recognition\n",
      "Token: or, Lemma: or\n",
      "Token: speech, Lemma: speech\n",
      "Token: recognition, Lemma: recognition\n",
      "Token: ., Lemma: .\n",
      "Token: In, Lemma: In\n",
      "Token: other, Lemma: other\n",
      "Token: neural, Lemma: neural\n",
      "Token: networks, Lemma: networks\n",
      "Token: ,, Lemma: ,\n",
      "Token: all, Lemma: all\n",
      "Token: the, Lemma: the\n",
      "Token: inputs, Lemma: input\n",
      "Token: are, Lemma: are\n",
      "Token: independent, Lemma: independent\n",
      "Token: of, Lemma: of\n",
      "Token: each, Lemma: each\n",
      "Token: other, Lemma: other\n",
      "Token: ., Lemma: .\n",
      "Token: But, Lemma: But\n",
      "Token: in, Lemma: in\n",
      "Token: RNN, Lemma: RNN\n",
      "Token: ,, Lemma: ,\n",
      "Token: all, Lemma: all\n",
      "Token: the, Lemma: the\n",
      "Token: inputs, Lemma: input\n",
      "Token: are, Lemma: are\n",
      "Token: related, Lemma: related\n",
      "Token: to, Lemma: to\n",
      "Token: each, Lemma: each\n",
      "Token: other, Lemma: other\n",
      "Token: ., Lemma: .\n",
      "Token: This, Lemma: This\n",
      "Token: helps, Lemma: helps\n",
      "Token: the, Lemma: the\n",
      "Token: RNNs, Lemma: RNNs\n",
      "Token: to, Lemma: to\n",
      "Token: exhibit, Lemma: exhibit\n",
      "Token: temporal, Lemma: temporal\n",
      "Token: dynamic, Lemma: dynamic\n",
      "Token: behavior, Lemma: behavior\n",
      "Token: ., Lemma: .\n",
      "Token: The, Lemma: The\n",
      "Token: memory, Lemma: memory\n",
      "Token: of, Lemma: of\n",
      "Token: the, Lemma: the\n",
      "Token: RNN, Lemma: RNN\n",
      "Token: is, Lemma: is\n",
      "Token: not, Lemma: not\n",
      "Token: pre-determined, Lemma: pre-determined\n",
      "Token: ., Lemma: .\n",
      "Token: It, Lemma: It\n",
      "Token: is, Lemma: is\n",
      "Token: a, Lemma: a\n",
      "Token: variable, Lemma: variable\n",
      "Token: that, Lemma: that\n",
      "Token: evolves, Lemma: evolves\n",
      "Token: over, Lemma: over\n",
      "Token: time, Lemma: time\n",
      "Token: ., Lemma: .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"Token: {token.text}, Lemma: {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Learnings :\n",
    "     1. use re for cleaning the text data\n",
    "     2. use nltk for stemming and lemmatization\n",
    "     3. transformers (21.1 s) are faster than normal lemmatization ( 1min 30.7 s) model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv= CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in range(len(sentences)):\n",
    "    review= re.sub('[^a-zA-Z]',' ',sentences[i])\n",
    "    review= review.lower()\n",
    "    review= review.split()\n",
    "    review= ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neural networks are set of algorithms which closely resemble the human brain and are designed to recognize patterns',\n",
       " 'they interpret sensory data through a machine perception labelling or clustering raw input',\n",
       " 'they can recognize numerical patterns contained in vectors into which all real world data images sound text or time series must be translated',\n",
       " 'artificial neural networks are composed of a large number of highly interconnected processing elements neuron working together to solve a problem',\n",
       " 'an ann usually involves a large number of processors operating in parallel and arranged in tiers',\n",
       " 'the first tier receives the raw input information analogous to optic nerves in human visual processing',\n",
       " 'each successive tier receives the output from the tier preceding it rather than from the raw input in the same way neurons further from the optic nerve receive signals from those closer to it',\n",
       " 'the last tier produces the output of the system recurrent neural network is a generalization of feedforward neural network that has an internal memory',\n",
       " 'rnn is recurrent in nature as it performs the same function for every input of data while the output of the current input depends on the past one computation',\n",
       " 'after producing the output it is copied and sent back into the recurrent network',\n",
       " 'for making a decision it considers the current input and the output that it has learned from the previous input',\n",
       " 'unlike feedforward neural networks rnns can use their internal state memory to process sequences of inputs',\n",
       " 'this makes them applicable to tasks such as unsegmented connected handwriting recognition or speech recognition',\n",
       " 'in other neural networks all the inputs are independent of each other',\n",
       " 'but in rnn all the inputs are related to each other',\n",
       " 'this helps the rnns to exhibit temporal dynamic behavior',\n",
       " 'the memory of the rnn is not pre determined',\n",
       " 'it is a variable that evolves over time']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=cv.fit_transform(corpus)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neural': 78,\n",
       " 'networks': 77,\n",
       " 'are': 8,\n",
       " 'set': 124,\n",
       " 'of': 84,\n",
       " 'algorithms': 1,\n",
       " 'which': 159,\n",
       " 'closely': 18,\n",
       " 'resemble': 116,\n",
       " 'the': 138,\n",
       " 'human': 50,\n",
       " 'brain': 15,\n",
       " 'and': 5,\n",
       " 'designed': 31,\n",
       " 'to': 148,\n",
       " 'recognize': 113,\n",
       " 'patterns': 95,\n",
       " 'they': 141,\n",
       " 'interpret': 59,\n",
       " 'sensory': 120,\n",
       " 'data': 28,\n",
       " 'through': 144,\n",
       " 'machine': 68,\n",
       " 'perception': 96,\n",
       " 'labelling': 64,\n",
       " 'or': 89,\n",
       " 'clustering': 20,\n",
       " 'raw': 108,\n",
       " 'input': 55,\n",
       " 'can': 17,\n",
       " 'numerical': 83,\n",
       " 'contained': 25,\n",
       " 'in': 52,\n",
       " 'vectors': 156,\n",
       " 'into': 60,\n",
       " 'all': 2,\n",
       " 'real': 109,\n",
       " 'world': 162,\n",
       " 'images': 51,\n",
       " 'sound': 127,\n",
       " 'text': 135,\n",
       " 'time': 147,\n",
       " 'series': 123,\n",
       " 'must': 72,\n",
       " 'be': 13,\n",
       " 'translated': 150,\n",
       " 'artificial': 10,\n",
       " 'composed': 21,\n",
       " 'large': 65,\n",
       " 'number': 82,\n",
       " 'highly': 49,\n",
       " 'interconnected': 57,\n",
       " 'processing': 103,\n",
       " 'elements': 35,\n",
       " 'neuron': 79,\n",
       " 'working': 161,\n",
       " 'together': 149,\n",
       " 'solve': 126,\n",
       " 'problem': 101,\n",
       " 'an': 3,\n",
       " 'ann': 6,\n",
       " 'usually': 154,\n",
       " 'involves': 61,\n",
       " 'processors': 104,\n",
       " 'operating': 87,\n",
       " 'parallel': 93,\n",
       " 'arranged': 9,\n",
       " 'tiers': 146,\n",
       " 'first': 40,\n",
       " 'tier': 145,\n",
       " 'receives': 111,\n",
       " 'information': 54,\n",
       " 'analogous': 4,\n",
       " 'optic': 88,\n",
       " 'nerves': 75,\n",
       " 'visual': 157,\n",
       " 'each': 34,\n",
       " 'successive': 130,\n",
       " 'output': 91,\n",
       " 'from': 42,\n",
       " 'preceding': 99,\n",
       " 'it': 63,\n",
       " 'rather': 107,\n",
       " 'than': 136,\n",
       " 'same': 119,\n",
       " 'way': 158,\n",
       " 'neurons': 80,\n",
       " 'further': 44,\n",
       " 'nerve': 74,\n",
       " 'receive': 110,\n",
       " 'signals': 125,\n",
       " 'those': 143,\n",
       " 'closer': 19,\n",
       " 'last': 66,\n",
       " 'produces': 105,\n",
       " 'system': 132,\n",
       " 'recurrent': 114,\n",
       " 'network': 76,\n",
       " 'is': 62,\n",
       " 'generalization': 45,\n",
       " 'feedforward': 39,\n",
       " 'that': 137,\n",
       " 'has': 47,\n",
       " 'internal': 58,\n",
       " 'memory': 71,\n",
       " 'rnn': 117,\n",
       " 'nature': 73,\n",
       " 'as': 11,\n",
       " 'performs': 97,\n",
       " 'function': 43,\n",
       " 'for': 41,\n",
       " 'every': 36,\n",
       " 'while': 160,\n",
       " 'current': 27,\n",
       " 'depends': 30,\n",
       " 'on': 85,\n",
       " 'past': 94,\n",
       " 'one': 86,\n",
       " 'computation': 22,\n",
       " 'after': 0,\n",
       " 'producing': 106,\n",
       " 'copied': 26,\n",
       " 'sent': 121,\n",
       " 'back': 12,\n",
       " 'making': 70,\n",
       " 'decision': 29,\n",
       " 'considers': 24,\n",
       " 'learned': 67,\n",
       " 'previous': 100,\n",
       " 'unlike': 151,\n",
       " 'rnns': 118,\n",
       " 'use': 153,\n",
       " 'their': 139,\n",
       " 'state': 129,\n",
       " 'process': 102,\n",
       " 'sequences': 122,\n",
       " 'inputs': 56,\n",
       " 'this': 142,\n",
       " 'makes': 69,\n",
       " 'them': 140,\n",
       " 'applicable': 7,\n",
       " 'tasks': 133,\n",
       " 'such': 131,\n",
       " 'unsegmented': 152,\n",
       " 'connected': 23,\n",
       " 'handwriting': 46,\n",
       " 'recognition': 112,\n",
       " 'speech': 128,\n",
       " 'other': 90,\n",
       " 'independent': 53,\n",
       " 'but': 16,\n",
       " 'related': 115,\n",
       " 'helps': 48,\n",
       " 'exhibit': 38,\n",
       " 'temporal': 134,\n",
       " 'dynamic': 33,\n",
       " 'behavior': 14,\n",
       " 'not': 81,\n",
       " 'pre': 98,\n",
       " 'determined': 32,\n",
       " 'variable': 155,\n",
       " 'evolves': 37,\n",
       " 'over': 92}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
